{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMoFlbmI0yokOY/WAo9JcrF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oraziotorre/MomentumShiftAI/blob/main/ModelsDevelopment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PRXc0RR1aR6-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importiamo il dataset ottenuto tramite le operazioni di Data Preprocessing\n",
        "dataset = pd.read_csv(\"tennis_2010-now.csv\")"
      ],
      "metadata": {
        "id": "UmDkY0i1jWrI"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split 'Pts'\n",
        "dataset[['Pt1_raw', 'Pt2_raw']] = dataset['Pts'].astype(str).str.split('-', expand=True)\n",
        "\n",
        "# Set numerici\n",
        "dataset['Gm1'] = pd.to_numeric(dataset['Gm1'], errors='coerce')\n",
        "dataset['Gm2'] = pd.to_numeric(dataset['Gm2'], errors='coerce')\n",
        "\n",
        "# Maschera tiebreak SOLO se 6-6\n",
        "is_tiebreak = (dataset['Gm1'] == 6) & (dataset['Gm2'] == 6)\n",
        "\n",
        "normal_score_map = {'0': 0, '15': 1, '30': 2, '40': 3, 'AD': 4}\n",
        "tiebreak_score_map = {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, 'AD': 7}\n",
        "\n",
        "pt1_normal = dataset['Pt1_raw'].map(normal_score_map)\n",
        "pt2_normal = dataset['Pt2_raw'].map(normal_score_map)\n",
        "\n",
        "pt1_tb = dataset['Pt1_raw'].map(tiebreak_score_map)\n",
        "pt2_tb = dataset['Pt2_raw'].map(tiebreak_score_map)\n",
        "\n",
        "dataset['Pt1'] = np.where(is_tiebreak, pt1_tb, pt1_normal)\n",
        "dataset['Pt2'] = np.where(is_tiebreak, pt2_tb, pt2_normal)\n",
        "\n",
        "# Definisci IsDeuce per game normale\n",
        "normal_deuce = (\n",
        "    (~is_tiebreak) &\n",
        "    (\n",
        "        ((dataset['Pt1'] == 3) & (dataset['Pt2'] == 3)) |   # 40-40\n",
        "        ((dataset['Pt1'] == 3) & (dataset['Pt2'] == 4)) |   # 40-ADV\n",
        "        ((dataset['Pt1'] == 4) & (dataset['Pt2'] == 3))     # ADV-40\n",
        "    )\n",
        ")\n",
        "\n",
        "# Definisci IsDeuce per tiebreak\n",
        "tiebreak_deuce = (\n",
        "    (is_tiebreak) &\n",
        "    (dataset['Pt1'] >= 6) &\n",
        "    (dataset['Pt2'] >= 6) &\n",
        "    (abs(dataset['Pt1'] - dataset['Pt2']) <= 1)\n",
        ")\n",
        "\n",
        "# Combina i due casi\n",
        "dataset['IsDeuce'] = (normal_deuce | tiebreak_deuce).astype(int)\n",
        "\n",
        "# Aggiungi colonna IsTieBreak\n",
        "dataset['IsTieBreak'] = is_tiebreak.astype(int)\n",
        "\n",
        "dataset.drop(columns=['Pt1_raw', 'Pt2_raw'], inplace=True)"
      ],
      "metadata": {
        "id": "CLAiBgf7jEYw"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.iloc[1470:1505][['Pts', 'Set1', 'Set2', 'Pt1', 'Pt2', 'IsDeuce','IsTieBreak']])"
      ],
      "metadata": {
        "id": "K1r0jtnUvx4e",
        "outputId": "b29bed20-2abe-4d1a-91c7-db2db755389e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Pts  Set1  Set2  Pt1  Pt2  IsDeuce  IsTieBreak\n",
            "1470    0-0     0     1  0.0  0.0        0           0\n",
            "1471   0-15     0     1  0.0  1.0        0           0\n",
            "1472  15-15     0     1  1.0  1.0        0           0\n",
            "1473  30-15     0     1  2.0  1.0        0           0\n",
            "1474  30-30     0     1  2.0  2.0        0           0\n",
            "1475  40-30     0     1  3.0  2.0        0           0\n",
            "1476  40-40     0     1  3.0  3.0        1           0\n",
            "1477  AD-40     0     1  4.0  3.0        1           0\n",
            "1478    0-0     0     1  0.0  0.0        0           0\n",
            "1479   15-0     0     1  1.0  0.0        0           0\n",
            "1480  15-15     0     1  1.0  1.0        0           0\n",
            "1481  15-30     0     1  1.0  2.0        0           0\n",
            "1482  30-30     0     1  2.0  2.0        0           0\n",
            "1483  30-40     0     1  2.0  3.0        0           0\n",
            "1484    0-0     0     1  0.0  0.0        0           1\n",
            "1485    0-1     0     1  0.0  1.0        0           1\n",
            "1486    1-1     0     1  1.0  1.0        0           1\n",
            "1487    1-2     0     1  1.0  2.0        0           1\n",
            "1488    1-3     0     1  1.0  3.0        0           1\n",
            "1489    2-3     0     1  2.0  3.0        0           1\n",
            "1490    2-4     0     1  2.0  4.0        0           1\n",
            "1491    2-5     0     1  2.0  5.0        0           1\n",
            "1492    3-5     0     1  3.0  5.0        0           1\n",
            "1493    4-5     0     1  4.0  5.0        0           1\n",
            "1494    4-6     0     1  4.0  6.0        0           1\n",
            "1495    5-6     0     1  5.0  6.0        0           1\n",
            "1496    6-6     0     1  6.0  6.0        1           1\n",
            "1497   AD-6     0     1  7.0  6.0        1           1\n",
            "1498    6-6     0     1  6.0  6.0        1           1\n",
            "1499   6-AD     0     1  6.0  7.0        1           1\n",
            "1500    6-6     0     1  6.0  6.0        1           1\n",
            "1501   6-AD     0     1  6.0  7.0        1           1\n",
            "1502    6-6     0     1  6.0  6.0        1           1\n",
            "1503   6-AD     0     1  6.0  7.0        1           1\n",
            "1504    6-6     0     1  6.0  6.0        1           1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.drop(columns=['PointType','Pts'])#,'Player1','Player2','SetID', 'SetWinner','PtSet'"
      ],
      "metadata": {
        "id": "l_RLfd4gqe6F"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Train-Test split**"
      ],
      "metadata": {
        "id": "d9iSSGWajlYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generazione delle partite inverse per aumentare la dimensione del dataset\n",
        "\n",
        "def augment_with_symmetric(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df_swapped = df.copy()\n",
        "\n",
        "    # Colonne da swappare a coppie\n",
        "    swap_pairs = [\n",
        "        ('Set1', 'Set2'),\n",
        "        ('Gm1', 'Gm2'),\n",
        "        ('Pt1', 'Pt2'),\n",
        "        ('Player1', 'Player2'),\n",
        "        ('Ranking1', 'Ranking2'),\n",
        "        ('p1_win_nobreak_point', 'p2_win_nobreak_point'),\n",
        "        ('p1_win_break_point', 'p2_win_break_point'),\n",
        "        ('p1_lost_nobreak_point', 'p2_lost_nobreak_point'),\n",
        "        ('p1_lost_break_point', 'p2_lost_break_point'),\n",
        "        ('serve_ace_1', 'serve_ace_2'),\n",
        "        ('serve_miss2_1', 'serve_miss1_2'),\n",
        "        ('rally_winner_1', 'rally_winner_2'),\n",
        "        ('rally_forced2_1', 'rally_forced1_2'),\n",
        "        ('rally_unforced1_2', 'rally_unforced2_1')\n",
        "    ]\n",
        "    for col1, col2 in swap_pairs:\n",
        "        df_swapped[[col1, col2]] = df[[col2, col1]].values\n",
        "\n",
        "    # Colonne dove 1 <-> 2\n",
        "    invert_1_2_cols = ['Svr', 'PtWinner', 'SetWinner', 'MatchWinner']\n",
        "    for col in invert_1_2_cols:\n",
        "        df_swapped[col] = df[col].replace({1: 2, 2: 1})\n",
        "\n",
        "\n",
        "    # Aggiunta del suffisso 'simm' a match_id e set_id, se presenti\n",
        "    for col in ['match_id', 'SetID']:\n",
        "        if col in df_swapped.columns:\n",
        "            df_swapped[col] = df_swapped[col].astype(str) + '_simm'\n",
        "\n",
        "    return pd.concat([df, df_swapped], ignore_index=True)"
      ],
      "metadata": {
        "id": "N61m-gtfXVZs"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1️⃣ Scaling e Normalizzazione\n",
        "\n",
        "def preprocess_data(train_data, test_data):\n",
        "    # Copie di sicurezza\n",
        "    train_data = train_data.copy()\n",
        "    test_data = test_data.copy()\n",
        "\n",
        "    # === 1. Colonne da trasformare ===\n",
        "    minmax_cols = ['Set1', 'Set2', 'Gm1', 'Gm2', 'Pt1', 'Pt2']\n",
        "    standard_cols = ['Ranking1', 'Ranking2']\n",
        "    others = ['Svr', 'PtWinner','IsDeuce','IsTieBreak']\n",
        "\n",
        "    # === 2. Altre feature da conservare (numeriche/categoriche già pronte) ===\n",
        "    exclude_cols = minmax_cols + standard_cols + others + \\\n",
        "                   ['match_id', 'SetID', 'Player1', 'Player2', 'PtSet', 'SetWinner', 'MatchWinner']\n",
        "\n",
        "    ################ other_features = [col for col in train_data.columns if col not in exclude_cols]\n",
        "\n",
        "    # === 3. Normalizzazione ===\n",
        "    minmax_scaler = MinMaxScaler()\n",
        "    train_data[minmax_cols] = minmax_scaler.fit_transform(train_data[minmax_cols])\n",
        "    test_data[minmax_cols] = minmax_scaler.transform(test_data[minmax_cols])\n",
        "\n",
        "    # === 4. Standardizzazione ===\n",
        "    standard_scaler = StandardScaler()\n",
        "    train_data[standard_cols] = standard_scaler.fit_transform(train_data[standard_cols])\n",
        "    test_data[standard_cols] = standard_scaler.transform(test_data[standard_cols])\n",
        "\n",
        "\n",
        "    feature_cols = minmax_cols +  others###### +standard_cols + pts_cols + svr_cols  + other_features\n",
        "\n",
        "    # === 8. Target: conversione [1,2] → [0,1] ===\n",
        "    target_cols = ['Svr', 'PtWinner', 'SetWinner', 'MatchWinner']\n",
        "    for col in target_cols:\n",
        "        train_data = train_data[train_data[col].isin([1, 2])]\n",
        "        test_data = test_data[test_data[col].isin([1, 2])]\n",
        "        train_data[col] = train_data[col].astype(int) - 1\n",
        "        test_data[col] = test_data[col].astype(int) - 1\n",
        "\n",
        "    # === 9. Conversione finale a float ===\n",
        "    train_data[feature_cols] = train_data[feature_cols]\n",
        "    test_data[feature_cols] = test_data[feature_cols]\n",
        "\n",
        "    return train_data, test_data, feature_cols"
      ],
      "metadata": {
        "id": "AskIfnUR2uZn"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2️⃣ Creazione sequenze\n",
        "\n",
        "def create_sequences(data, feature_cols, target_col='SetWinner'):\n",
        "    X_sequences = []\n",
        "    y_labels = []\n",
        "    lengths = []\n",
        "\n",
        "    # Ordina i punti all'interno di ciascun match\n",
        "    data = data.sort_values(['SetID', 'PtSet']).reset_index(drop=True)\n",
        "\n",
        "    for match_id, df_match in data.groupby('SetID'):\n",
        "\n",
        "        seq_data = df_match[feature_cols].to_numpy(dtype=np.float32)\n",
        "        X_seq = torch.tensor(seq_data, dtype=torch.float32)\n",
        "        X_sequences.append(X_seq)\n",
        "\n",
        "        # Prende il target del primo punto del match (assunto costante per tutti)\n",
        "        target_winner = int(df_match[target_col].iloc[0])\n",
        "        y_labels.append(target_winner)\n",
        "        lengths.append(len(df_match))\n",
        "        '''\n",
        "        # Stampa la sequenza e il target per il SetID corrente\n",
        "        print(f\"SetID: {match_id}\")\n",
        "        print(\"Sequence (features):\")\n",
        "        print(X_seq)\n",
        "        print(\"Target (SetWinner):\", target_winner)\n",
        "        print(\"-\" * 40)\n",
        "        '''\n",
        "    # Padding delle sequenze\n",
        "    X_padded = pad_sequence(X_sequences, batch_first=True)\n",
        "    y_tensor = torch.tensor(y_labels, dtype=torch.long)\n",
        "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
        "\n",
        "    return X_padded, y_tensor, lengths"
      ],
      "metadata": {
        "id": "CJdb-s2_s3fS"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3️⃣ Pipeline completa + DataLoader\n",
        "\n",
        "def prepare_datasets(train_data, test_data, target_col='SetWinner', batch_size=32):\n",
        "    # Preprocessing dati\n",
        "    train_proc, test_proc, feature_cols = preprocess_data(train_data, test_data)\n",
        "    # Creazione sequenze\n",
        "    X_train, y_train, train_lengths = create_sequences(train_proc, feature_cols, target_col)\n",
        "    X_test, y_test, test_lengths = create_sequences(test_proc, feature_cols, target_col)\n",
        "\n",
        "    # Costruzione dei TensorDataset\n",
        "    train_dataset = TensorDataset(X_train, y_train, train_lengths)\n",
        "    test_dataset = TensorDataset(X_test, y_test, test_lengths)\n",
        "\n",
        "\n",
        "    # Funzione per ordinare le sequenze per lunghezza (richiesto da LSTM)\n",
        "    def collate_fn(batch):\n",
        "        inputs, targets, lengths = zip(*batch)\n",
        "        inputs = torch.stack(inputs)\n",
        "        targets = torch.stack(targets)\n",
        "        lengths = torch.stack(lengths)\n",
        "        sorted_idx = torch.argsort(lengths, descending=True)\n",
        "        return inputs[sorted_idx], targets[sorted_idx], lengths[sorted_idx]\n",
        "\n",
        "\n",
        "    # DataLoader\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    return train_loader, test_loader, feature_cols"
      ],
      "metadata": {
        "id": "PhGU24mJd98Y"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Estrai tutti gli ID unici delle partite\n",
        "match_ids = dataset['match_id'].unique()\n",
        "\n",
        "# Split degli ID in training e test (es. 80% train, 20% test)\n",
        "train_ids, test_ids = train_test_split(match_ids, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split per match-id\n",
        "train_data = dataset[dataset['match_id'].isin(train_ids)].reset_index(drop=True)\n",
        "test_data = dataset[dataset['match_id'].isin(test_ids)].reset_index(drop=True)\n",
        "\n",
        "#train_data = augment_with_symmetric(train_data)\n",
        "#test_data = augment_with_symmetric(test_data)\n",
        "\n",
        "train_loader, test_loader, feature_cols = prepare_datasets(train_data, test_data, target_col='SetWinner', batch_size=32)\n",
        "feature_cols"
      ],
      "metadata": {
        "id": "DaJqb-9rfTY0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98bb46fa-f3ad-4db0-e02d-18433735f867"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Set1', 'Set2', 'Gm1', 'Gm2', 'Pt1', 'Pt2', 'Svr', 'PtWinner', 'IsDeuce']"
            ]
          },
          "metadata": {},
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Estrai il primo batch dal DataLoader\n",
        "batch = next(iter(train_loader))\n",
        "\n",
        "# Stampa la struttura del batch\n",
        "print(type(batch))         # Di solito è una tupla o un dizionario\n",
        "print(len(batch))          # Quanti elementi contiene (es. (input, label) => 2)\n",
        "print(batch)"
      ],
      "metadata": {
        "id": "JXo-EHNwYkSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Model 1**"
      ],
      "metadata": {
        "id": "BQLPMefGkocP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RealTimeLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
        "                           batch_first=True, dropout=dropout)\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 1),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_size, 2)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        # Packing sequence\n",
        "        packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        # LSTM\n",
        "        packed_out, _ = self.lstm(packed)\n",
        "        out, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
        "\n",
        "        # Attention mechanism\n",
        "        weights = self.attention(out)\n",
        "        context = torch.sum(weights * out, dim=1)\n",
        "\n",
        "        # Classification\n",
        "        return self.fc(context)\n",
        "\n",
        "# 2️⃣ Funzione di training e validazione\n",
        "def train_model(model, train_loader, test_loader, learning_rate=0.001, epochs=20, patience=3):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='max',\n",
        "        factor=0.5,\n",
        "        patience=2,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    best_accuracy = 0\n",
        "    counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for inputs, labels, lengths in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs, lengths)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels, lengths in test_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(inputs, lengths)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "        # Calculate metrics\n",
        "        train_loss = train_loss / len(train_loader.dataset)\n",
        "        val_loss = val_loss / len(test_loader.dataset)\n",
        "        accuracy = 100 * correct / total\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}:')\n",
        "        print(f'Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "        # Early stopping\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            counter = 0\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "            print('Model saved!')\n",
        "        else:\n",
        "            counter += 1\n",
        "            if counter >= patience:\n",
        "                print(f'Early stopping at epoch {epoch+1}')\n",
        "                break\n",
        "\n",
        "        scheduler.step(accuracy)\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Parametri ragionevoli\n",
        "input_size = len(feature_cols)  # ~5-7 features\n",
        "hidden_size = 64\n",
        "num_layers = 1\n",
        "\n",
        "# Addestra con early stopping\n",
        "train_model(model, train_loader, test_loader,\n",
        "            learning_rate=1e-3,\n",
        "            patience=5,\n",
        "            epochs=50)"
      ],
      "metadata": {
        "id": "UkEA6olwvjSL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "ec5ed9a5-889c-4ed1-d17d-228c2daf6e82"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-196-2034325074.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;31m# Addestra con early stopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m train_model(model, train_loader, test_loader, \n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-196-2034325074.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, learning_rate, epochs, patience)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_profile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/profiler.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_callbacks_on_exit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model 2**"
      ],
      "metadata": {
        "id": "EnAZeV1LlUMe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KH1rrabmlWDC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}