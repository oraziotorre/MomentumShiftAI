{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO7Es0XHQnb7WzrIjIhdNCH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oraziotorre/MomentumShiftAI/blob/main/ModelsDevelopment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PRXc0RR1aR6-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importiamo il dataset ottenuto tramite le operazioni di Data Preprocessing\n",
        "dataset = pd.read_csv(\"tennis.csv\")"
      ],
      "metadata": {
        "id": "UmDkY0i1jWrI"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split 'Pts'\n",
        "dataset[['Pt1_raw', 'Pt2_raw']] = dataset['Pts'].astype(str).str.split('-', expand=True)\n",
        "\n",
        "# Set numerici\n",
        "dataset['Gm1'] = pd.to_numeric(dataset['Gm1'], errors='coerce')\n",
        "dataset['Gm2'] = pd.to_numeric(dataset['Gm2'], errors='coerce')\n",
        "\n",
        "# Maschera tiebreak SOLO se 6-6\n",
        "is_tiebreak = (dataset['Gm1'] == 6) & (dataset['Gm2'] == 6)\n",
        "\n",
        "normal_score_map = {'0': 0, '15': 1, '30': 2, '40': 3, 'AD': 4}\n",
        "tiebreak_score_map = {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, 'AD': 7}\n",
        "\n",
        "pt1_normal = dataset['Pt1_raw'].map(normal_score_map)\n",
        "pt2_normal = dataset['Pt2_raw'].map(normal_score_map)\n",
        "\n",
        "pt1_tb = dataset['Pt1_raw'].map(tiebreak_score_map)\n",
        "pt2_tb = dataset['Pt2_raw'].map(tiebreak_score_map)\n",
        "\n",
        "dataset['Pt1'] = np.where(is_tiebreak, pt1_tb, pt1_normal)\n",
        "dataset['Pt2'] = np.where(is_tiebreak, pt2_tb, pt2_normal)\n",
        "\n",
        "# Definisci IsDeuce per game normale\n",
        "normal_deuce = (\n",
        "    (~is_tiebreak) &\n",
        "    (\n",
        "        ((dataset['Pt1'] == 3) & (dataset['Pt2'] == 3)) |   # 40-40\n",
        "        ((dataset['Pt1'] == 3) & (dataset['Pt2'] == 4)) |   # 40-ADV\n",
        "        ((dataset['Pt1'] == 4) & (dataset['Pt2'] == 3))     # ADV-40\n",
        "    )\n",
        ")\n",
        "\n",
        "# Definisci IsDeuce per tiebreak\n",
        "tiebreak_deuce = (\n",
        "    (is_tiebreak) &\n",
        "    (dataset['Pt1'] >= 6) &\n",
        "    (dataset['Pt2'] >= 6) &\n",
        "    (abs(dataset['Pt1'] - dataset['Pt2']) <= 1)\n",
        ")\n",
        "\n",
        "# Combina i due casi\n",
        "dataset['IsDeuce'] = (normal_deuce | tiebreak_deuce).astype(int)\n",
        "\n",
        "# Aggiungi colonna IsTieBreak\n",
        "dataset['IsTieBreak'] = is_tiebreak.astype(int)\n",
        "\n",
        "dataset.drop(columns=['Pt1_raw', 'Pt2_raw'], inplace=True)"
      ],
      "metadata": {
        "id": "CLAiBgf7jEYw"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.iloc[1470:1505][['Pts', 'Set1', 'Set2', 'Pt1', 'Pt2', 'IsDeuce','IsTieBreak']])"
      ],
      "metadata": {
        "id": "K1r0jtnUvx4e",
        "outputId": "f008b2de-e986-4d22-e7f0-196669afc0ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Pts  Set1  Set2  Pt1  Pt2  IsDeuce  IsTieBreak\n",
            "1470    0-0     0     1  0.0  0.0        0           0\n",
            "1471   0-15     0     1  0.0  1.0        0           0\n",
            "1472  15-15     0     1  1.0  1.0        0           0\n",
            "1473  30-15     0     1  2.0  1.0        0           0\n",
            "1474  30-30     0     1  2.0  2.0        0           0\n",
            "1475  40-30     0     1  3.0  2.0        0           0\n",
            "1476  40-40     0     1  3.0  3.0        1           0\n",
            "1477  AD-40     0     1  4.0  3.0        1           0\n",
            "1478    0-0     0     1  0.0  0.0        0           0\n",
            "1479   15-0     0     1  1.0  0.0        0           0\n",
            "1480  15-15     0     1  1.0  1.0        0           0\n",
            "1481  15-30     0     1  1.0  2.0        0           0\n",
            "1482  30-30     0     1  2.0  2.0        0           0\n",
            "1483  30-40     0     1  2.0  3.0        0           0\n",
            "1484    0-0     0     1  0.0  0.0        0           1\n",
            "1485    0-1     0     1  0.0  1.0        0           1\n",
            "1486    1-1     0     1  1.0  1.0        0           1\n",
            "1487    1-2     0     1  1.0  2.0        0           1\n",
            "1488    1-3     0     1  1.0  3.0        0           1\n",
            "1489    2-3     0     1  2.0  3.0        0           1\n",
            "1490    2-4     0     1  2.0  4.0        0           1\n",
            "1491    2-5     0     1  2.0  5.0        0           1\n",
            "1492    3-5     0     1  3.0  5.0        0           1\n",
            "1493    4-5     0     1  4.0  5.0        0           1\n",
            "1494    4-6     0     1  4.0  6.0        0           1\n",
            "1495    5-6     0     1  5.0  6.0        0           1\n",
            "1496    6-6     0     1  6.0  6.0        1           1\n",
            "1497   AD-6     0     1  7.0  6.0        1           1\n",
            "1498    6-6     0     1  6.0  6.0        1           1\n",
            "1499   6-AD     0     1  6.0  7.0        1           1\n",
            "1500    6-6     0     1  6.0  6.0        1           1\n",
            "1501   6-AD     0     1  6.0  7.0        1           1\n",
            "1502    6-6     0     1  6.0  6.0        1           1\n",
            "1503   6-AD     0     1  6.0  7.0        1           1\n",
            "1504    6-6     0     1  6.0  6.0        1           1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.drop(columns=['PointType','Pts'])#,'Player1','Player2','SetID', 'SetWinner','PtSet'"
      ],
      "metadata": {
        "id": "l_RLfd4gqe6F"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Train-Test split**"
      ],
      "metadata": {
        "id": "d9iSSGWajlYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generazione delle partite inverse per aumentare la dimensione del dataset\n",
        "\n",
        "def augment_with_symmetric(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df_swapped = df.copy()\n",
        "\n",
        "    # Colonne da swappare a coppie\n",
        "    swap_pairs = [\n",
        "        ('Set1', 'Set2'),\n",
        "        ('Gm1', 'Gm2'),\n",
        "        ('Pt1', 'Pt2'),\n",
        "        ('Player1', 'Player2'),\n",
        "        ('Ranking1', 'Ranking2'),\n",
        "        ('p1_win_nobreak_point', 'p2_win_nobreak_point'),\n",
        "        ('p1_win_break_point', 'p2_win_break_point'),\n",
        "        ('p1_lost_nobreak_point', 'p2_lost_nobreak_point'),\n",
        "        ('p1_lost_break_point', 'p2_lost_break_point'),\n",
        "        ('serve_ace_1', 'serve_ace_2'),\n",
        "        ('serve_miss2_1', 'serve_miss1_2'),\n",
        "        ('rally_winner_1', 'rally_winner_2'),\n",
        "        ('rally_forced2_1', 'rally_forced1_2'),\n",
        "        ('rally_unforced1_2', 'rally_unforced2_1')\n",
        "    ]\n",
        "    for col1, col2 in swap_pairs:\n",
        "        df_swapped[[col1, col2]] = df[[col2, col1]].values\n",
        "\n",
        "    # Colonne dove 1 <-> 2\n",
        "    invert_1_2_cols = ['Svr', 'PtWinner', 'SetWinner', 'MatchWinner']\n",
        "    for col in invert_1_2_cols:\n",
        "        df_swapped[col] = df[col].replace({1: 2, 2: 1})\n",
        "\n",
        "\n",
        "    # Aggiunta del suffisso 'simm' a match_id e set_id, se presenti\n",
        "    for col in ['match_id', 'SetID']:\n",
        "        if col in df_swapped.columns:\n",
        "            df_swapped[col] = df_swapped[col].astype(str) + '_simm'\n",
        "\n",
        "    return pd.concat([df, df_swapped], ignore_index=True)"
      ],
      "metadata": {
        "id": "N61m-gtfXVZs"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1️⃣ Scaling e Normalizzazione\n",
        "\n",
        "def preprocess_data(train_data, test_data):\n",
        "    # Copie di sicurezza\n",
        "    train_data = train_data.copy()\n",
        "    test_data = test_data.copy()\n",
        "\n",
        "    # === 1. Colonne da trasformare ===\n",
        "    minmax_cols = ['Gm1', 'Gm2', 'Pt1', 'Pt2']\n",
        "    standard_cols = ['Ranking1', 'Ranking2']\n",
        "    others = [\n",
        "    'Svr', 'PtWinner', 'IsDeuce', 'IsTieBreak',\n",
        "    'p1_win_nobreak_point', 'p2_win_nobreak_point',\n",
        "    'p1_win_break_point', 'p2_win_break_point',\n",
        "    'p1_lost_nobreak_point', 'p2_lost_nobreak_point',\n",
        "    'p1_lost_break_point', 'p2_lost_break_point',\n",
        "    'serve_ace_1', 'serve_ace_2',\n",
        "    'serve_miss2_1', 'serve_miss1_2',\n",
        "    'rally_winner_1', 'rally_winner_2',\n",
        "    'rally_forced2_1', 'rally_forced1_2',\n",
        "    'rally_unforced1_2', 'rally_unforced2_1'\n",
        "    ]\n",
        "\n",
        "    # === 3. Normalizzazione ===\n",
        "    minmax_scaler = MinMaxScaler()\n",
        "    train_data[minmax_cols] = minmax_scaler.fit_transform(train_data[minmax_cols])\n",
        "    test_data[minmax_cols] = minmax_scaler.transform(test_data[minmax_cols])\n",
        "\n",
        "    # === 4. Standardizzazione ===\n",
        "    standard_scaler = StandardScaler()\n",
        "    train_data[standard_cols] = standard_scaler.fit_transform(train_data[standard_cols])\n",
        "    test_data[standard_cols] = standard_scaler.transform(test_data[standard_cols])\n",
        "\n",
        "    feature_cols = minmax_cols + others\n",
        "\n",
        "    # === 5. Target: conversione [1,2] → [0,1] ===\n",
        "    target_cols = ['Svr', 'PtWinner', 'SetWinner', 'MatchWinner']\n",
        "    for col in target_cols:\n",
        "        train_data = train_data[train_data[col].isin([1, 2])]\n",
        "        test_data = test_data[test_data[col].isin([1, 2])]\n",
        "        train_data[col] = train_data[col].astype(int) - 1\n",
        "        test_data[col] = test_data[col].astype(int) - 1\n",
        "\n",
        "    # === 6. Conversione finale a float ===\n",
        "    train_data[feature_cols] = train_data[feature_cols].astype(float)\n",
        "    test_data[feature_cols] = test_data[feature_cols].astype(float)\n",
        "\n",
        "    return train_data, test_data, feature_cols"
      ],
      "metadata": {
        "id": "AskIfnUR2uZn"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2️⃣ Creazione sequenze\n",
        "\n",
        "def create_sequences(data, feature_cols, target_col='SetWinner'):\n",
        "    X_sequences = []\n",
        "    y_labels = []\n",
        "    lengths = []\n",
        "\n",
        "    # Ordina i punti all'interno di ciascun match\n",
        "    data = data.sort_values(['SetID', 'PtSet']).reset_index(drop=True)\n",
        "\n",
        "    for match_id, df_match in data.groupby('SetID'):\n",
        "        seq_data = df_match[feature_cols].to_numpy(dtype=np.float32)\n",
        "        X_seq = torch.tensor(seq_data, dtype=torch.float32)\n",
        "        X_sequences.append(X_seq)\n",
        "\n",
        "        # Prende il target del primo punto del match (assunto costante per tutti)\n",
        "        target_winner = int(df_match[target_col].iloc[0])\n",
        "        y_labels.append(target_winner)\n",
        "        lengths.append(len(df_match))\n",
        "\n",
        "    # Padding delle sequenze\n",
        "    X_padded = pad_sequence(X_sequences, batch_first=True, padding_value=0)\n",
        "    y_tensor = torch.tensor(y_labels, dtype=torch.float)\n",
        "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
        "\n",
        "    return X_padded, y_tensor, lengths"
      ],
      "metadata": {
        "id": "CJdb-s2_s3fS"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3️⃣ Pipeline completa + DataLoader\n",
        "\n",
        "def collate_fn(batch):\n",
        "    inputs, targets, lengths = zip(*batch)\n",
        "    inputs = torch.stack(inputs)\n",
        "    targets = torch.stack(targets)\n",
        "    lengths = torch.stack(lengths)\n",
        "    sorted_idx = torch.argsort(lengths, descending=True)\n",
        "    return inputs[sorted_idx], targets[sorted_idx], lengths[sorted_idx]\n",
        "\n",
        "def prepare_datasets(train_data, test_data, target_col='SetWinner', batch_size=32, val_split=0.1, random_state=42):\n",
        "    # Preprocessing dati\n",
        "    train_proc, test_proc, feature_cols = preprocess_data(train_data, test_data)\n",
        "\n",
        "    # Split ulteriore del training set in train/validation\n",
        "    match_ids = train_proc['SetID'].unique()\n",
        "    train_ids, val_ids = train_test_split(match_ids, test_size=val_split, random_state=random_state)\n",
        "\n",
        "    train_split = train_proc[train_proc['SetID'].isin(train_ids)]\n",
        "    val_split = train_proc[train_proc['SetID'].isin(val_ids)]\n",
        "\n",
        "    # Creazione sequenze per train/val/test\n",
        "    X_train, y_train, train_lengths = create_sequences(train_split, feature_cols, target_col)\n",
        "    X_val, y_val, val_lengths = create_sequences(val_split, feature_cols, target_col)\n",
        "    X_test, y_test, test_lengths = create_sequences(test_proc, feature_cols, target_col)\n",
        "\n",
        "    # Costruzione dei TensorDataset\n",
        "    train_dataset = TensorDataset(X_train, y_train, train_lengths)\n",
        "    val_dataset = TensorDataset(X_val, y_val, val_lengths)\n",
        "    test_dataset = TensorDataset(X_test, y_test, test_lengths)\n",
        "\n",
        "    # DataLoader\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                             shuffle=True, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size,\n",
        "                           shuffle=False, collate_fn=collate_fn)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
        "                            shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    return train_loader, val_loader, test_loader, feature_cols"
      ],
      "metadata": {
        "id": "PhGU24mJd98Y"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Model 1**"
      ],
      "metadata": {
        "id": "BQLPMefGkocP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 1. MODELLO AGGIORNATO ----------\n",
        "class LSTMPerStep(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers,\n",
        "                            batch_first=True, bidirectional=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.output_layer = nn.Linear(hidden_size, 1)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def forward(self, x, lengths, hidden=None):\n",
        "        lengths = lengths.cpu()\n",
        "        packed = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
        "        packed_output, hidden = self.lstm(packed, hidden)\n",
        "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
        "        output = self.dropout(output)\n",
        "        logits = self.output_layer(output)  # (batch, seq_len, 1)\n",
        "        probs = torch.sigmoid(logits).squeeze(-1)  # (batch, seq_len)\n",
        "        return probs, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size, device):\n",
        "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device),\n",
        "                torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device))\n",
        "\n",
        "# ---------- 2. NUOVA LOSS PESATA ----------\n",
        "def exponential_weighted_loss(y_pred, y_true, lengths, device, final_weight=10.0):\n",
        "    \"\"\"\n",
        "    Loss che aumenta esponenzialmente il peso verso la fine della sequenza\n",
        "    \"\"\"\n",
        "    batch_size, max_len = y_pred.shape\n",
        "    weights = torch.ones_like(y_pred, device=device)\n",
        "\n",
        "    # Crea pesi esponenziali per ogni sequenza nel batch\n",
        "    for i in range(batch_size):\n",
        "        L = lengths[i].item()\n",
        "        # Peso base cresce esponenzialmente verso la fine\n",
        "        t = torch.arange(0, L, device=device).float()\n",
        "        sequence_weights = torch.exp(final_weight * (t / L - 1))\n",
        "\n",
        "        # Normalizza per mantenere l'impatto complessivo simile\n",
        "        sequence_weights = sequence_weights / sequence_weights.mean()\n",
        "        weights[i, :L] = sequence_weights\n",
        "\n",
        "    # Calcola BCE pesata\n",
        "    loss = F.binary_cross_entropy(y_pred, y_true, reduction='none')\n",
        "    weighted_loss = loss * weights\n",
        "\n",
        "    # Calcola solo sulle posizioni valide\n",
        "    mask = torch.arange(max_len, device=device)[None, :] < lengths[:, None]\n",
        "    valid_loss = weighted_loss[mask].mean()\n",
        "\n",
        "    return valid_loss\n",
        "\n",
        "# ---------- 3. TRAINING LOOP RIVISTO ----------\n",
        "def train(model, train_loader, val_loader, optimizer, device, epochs=50, patience=5):\n",
        "    best_val_loss = float('inf')\n",
        "    counter = 0\n",
        "    history = {'train_loss': [], 'val_loss': []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for X_batch, y_batch, lengths in train_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            lengths = lengths.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            preds, _ = model(X_batch, lengths)\n",
        "\n",
        "            # Crea target espansi per ogni passo temporale\n",
        "            y_expanded = y_batch.unsqueeze(1).repeat(1, preds.size(1))\n",
        "\n",
        "            # Calcola loss\n",
        "            loss = exponential_weighted_loss(preds, y_expanded, lengths, device)\n",
        "\n",
        "            # Backpropagazione\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Validation\n",
        "        val_loss = 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch, lengths in val_loader:\n",
        "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "                lengths = lengths.to(device)\n",
        "\n",
        "                preds, _ = model(X_batch, lengths)\n",
        "                y_expanded = y_batch.unsqueeze(1).repeat(1, preds.size(1))\n",
        "                loss = exponential_weighted_loss(preds, y_expanded, lengths, device)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        # Calcola medie\n",
        "        train_loss /= len(train_loader)\n",
        "        val_loss /= len(val_loader)\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            counter = 0\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "        else:\n",
        "            counter += 1\n",
        "            if counter >= patience:\n",
        "                print(f'Early stopping at epoch {epoch+1}')\n",
        "                break\n",
        "\n",
        "    # Carica il miglior modello\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "    return history\n",
        "\n",
        "# ---------- 4. PREDIZIONE EFFICIENTE CON STATO PERSISTENTE ----------\n",
        "def predict_sequence_steps(model, X_seq, device):\n",
        "    model.eval()\n",
        "    X_seq = X_seq.unsqueeze(0).to(device)  # [1, seq_len, input_dim]\n",
        "    seq_len = X_seq.size(1)\n",
        "    probs = []\n",
        "\n",
        "    # Inizializza lo stato nascosto\n",
        "    hidden = model.init_hidden(1, device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Elabora la sequenza incrementale\n",
        "        for t in range(1, seq_len + 1):\n",
        "            input_step = X_seq[:, :t, :]\n",
        "            lengths = torch.tensor([t], dtype=torch.long).to(device)\n",
        "\n",
        "            # Passa attraverso il modello mantenendo lo stato\n",
        "            output, hidden = model(input_step, lengths, hidden)\n",
        "            prob = output[0, -1].item()\n",
        "            probs.append(prob)\n",
        "\n",
        "            # Stampa diagnostica\n",
        "            print(f\"Passo {t}/{seq_len}:\")\n",
        "            print(f\"Feature: {input_step[0, -1, :].cpu().numpy().round(3)}\")\n",
        "            print(f\"Probabilità: {prob:.4f}\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "    return probs\n",
        "\n",
        "# ---------- 5. ESEMPIO DI UTILIZZO AGGIORNATO ----------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_size = 26\n",
        "hidden_size = 64\n",
        "\n",
        "model = LSTMPerStep(input_size=input_size, hidden_size=hidden_size).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "# Preparazione dati (assumendo che 'dataset' sia già definito)\n",
        "match_ids = dataset['match_id'].unique()\n",
        "train_ids, test_ids = train_test_split(match_ids, test_size=0.2, random_state=42)\n",
        "train_data = dataset[dataset['match_id'].isin(train_ids)]\n",
        "test_data = dataset[dataset['match_id'].isin(test_ids)]\n",
        "\n",
        "train_data = augment_with_symmetric(train_data)\n",
        "test_data = augment_with_symmetric(test_data)\n",
        "\n",
        "# Creazione DataLoader (implementa la tua funzione prepare_datasets)\n",
        "train_loader, val_loader, test_loader, feature_cols = prepare_datasets(\n",
        "    train_data, test_data, target_col='SetWinner', batch_size=32, val_split=0.1\n",
        ")\n",
        "\n",
        "# Allenamento\n",
        "history = train(model, train_loader, val_loader, optimizer, device, epochs=100)\n",
        "\n",
        "# Plot della loss\n",
        "plt.plot(history['train_loss'], label='Train Loss')\n",
        "plt.plot(history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training Progress')\n",
        "plt.show()\n",
        "\n",
        "# Predizione su una sequenza di test\n",
        "X_test, y_test, lengths = next(iter(test_loader))\n",
        "X_seq = X_test[0][:lengths[0]]  # Sequenza completa\n",
        "\n",
        "print(f\"Etichetta vera: {int(y_test[0].item())}\")\n",
        "full_probs = predict_sequence_steps(model, X_seq, device)\n",
        "\n",
        "# Plot evoluzione probabilità\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, len(full_probs) + 1), full_probs, marker='o')\n",
        "plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5)\n",
        "plt.xlabel(\"Numero di punti osservati\")\n",
        "plt.ylabel(\"Probabilità predetta\")\n",
        "plt.title(\"Evoluzione della predizione\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "dqG7CbdvlwLg",
        "outputId": "03faeb28-c8c6-4d66-ca2d-d3b77e02cc6e"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-153-357563097.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;31m# Creazione DataLoader (implementa la tua funzione prepare_datasets)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m train_loader, val_loader, test_loader, feature_cols = prepare_datasets(\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'SetWinner'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m )\n",
            "\u001b[0;32m/tmp/ipython-input-152-1853146089.py\u001b[0m in \u001b[0;36mprepare_datasets\u001b[0;34m(train_data, test_data, target_col, batch_size, val_split, random_state)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Creazione sequenze per train/val/test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_proc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-151-2622007543.py\u001b[0m in \u001b[0;36mcreate_sequences\u001b[0;34m(data, feature_cols, target_col)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmatch_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_match\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SetID'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mseq_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_match\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mX_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mX_sequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4115\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4117\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_single_key\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_take_with_is_copy\u001b[0;34m(self, indices, axis)\u001b[0m\n\u001b[1;32m   4151\u001b[0m         \u001b[0mSee\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocstring\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfull\u001b[0m \u001b[0mexplanation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4152\u001b[0m         \"\"\"\n\u001b[0;32m-> 4153\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4154\u001b[0m         \u001b[0;31m# Maybe set copy if we didn't actually change the index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[1;32m   4131\u001b[0m             )\n\u001b[1;32m   4132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4133\u001b[0;31m         new_data = self._mgr.take(\n\u001b[0m\u001b[1;32m   4134\u001b[0m             \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4135\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[0mnew_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         return self.reindex_indexer(\n\u001b[0m\u001b[1;32m    895\u001b[0m             \u001b[0mnew_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m             \u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m             new_blocks = self._slice_take_blocks_ax0(\n\u001b[0m\u001b[1;32m    681\u001b[0m                 \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m                 \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_slice_take_blocks_ax0\u001b[0;34m(self, slice_or_indexer, fill_value, only_slice, use_na_proxy, ref_inplace_op)\u001b[0m\n\u001b[1;32m    783\u001b[0m             \u001b[0mblklocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblklocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslobj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m             blknos = algos.take_nd(\n\u001b[0m\u001b[1;32m    786\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblknos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/array_algos/take.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_take_nd_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/array_algos/take.py\u001b[0m in \u001b[0;36m_take_nd_ndarray\u001b[0;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_platform_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     dtype, fill_value, mask_info = _take_preprocess_indexer_and_fill_value(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/array_algos/take.py\u001b[0m in \u001b[0;36m_take_preprocess_indexer_and_fill_value\u001b[0;34m(arr, indexer, fill_value, allow_fill, mask)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;31m# check for promotion based on types only (do this first because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;31m# it's faster than computing a mask)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_promote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0;31m# check if promotion is actually required based on indexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mmaybe_promote\u001b[0;34m(dtype, fill_value)\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[0morig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[0morig_is_nat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mchecknull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m         \u001b[0;31m# https://github.com/pandas-dev/pandas/pull/39692#issuecomment-1441051740\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;31m#  avoid cache misses with NaN/NaT values that are not singletons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prob(p, gx, gy, x, y):\n",
        "\n",
        "  if gx == 6 and gy == 6:\n",
        "        sum_part1 = 0\n",
        "        for i in range(0, 6 - y):\n",
        "          sum_part1 += p**(7 - x) * math.comb(6 + i - x, i) * (1 - p)**i\n",
        "\n",
        "        # Parte via deuce da 10-10\n",
        "        part2 = p**(6 - x) * math.comb(12 - x - y, 6 - x) * (1 - p)**(6 - y)\n",
        "        d = (p**2) / (1 - 2 * p * (1 - p))  # Probabilità di vincere da deuce\n",
        "        part2 *= d\n",
        "\n",
        "        total = sum_part1 + part2\n",
        "        return total\n",
        "'''\n",
        "  else:\n",
        "\n",
        "#IN FASE DI SVILUPPO\n",
        "      # Caso base: il giocatore ha già vinto 11 e con almeno 2 di vantaggio\n",
        "      if x >= 4 and x - y >= 2:\n",
        "          return 1.0\n",
        "      elif y >= 4 and y - x >= 2:\n",
        "          return 0.0\n",
        "\n",
        "      # Probabilità di vincere game attuale\n",
        "      sum_part1 = 0\n",
        "      for i in range(0, 3 - y):\n",
        "        sum_part1 += p**(4 - x) * math.comb(3 + i - x, i) * (1 - p)**i\n",
        "\n",
        "      # Parte via deuce da 10-10\n",
        "      part2 = p**(3 - x) * math.comb(6 - x - y, 3 - x) * (1 - p)**(3 - y)\n",
        "      d = (p**2) / (1 - 2 * p * (1 - p))  # Probabilità di vincere da deuce\n",
        "      part2 *= d\n",
        "\n",
        "      game = sum_part1 + part2\n",
        "\n",
        "\n",
        "\n",
        "      # Probabilità di vincere il set dai game attuale gx e gy\n",
        "\n",
        "      sum_part1 = 0\n",
        "      for i in range(0, 5 - gy):\n",
        "        sum_part1 += p**(6 - gx) * math.comb(5 + i - gx, i) * (1 - p)**i\n",
        "\n",
        "      part2 = p**(5 - gx) * math.comb(10 - gx - gy, 5 - x) * (1 - p)**(5 - gy)\n",
        "      d = (p**2) / (1 - 2 * p * (1 - p))  # Probabilità di vincere da deuce\n",
        "      part2 *= d\n",
        "\n",
        "      total = sum_part1 + part2\n",
        "      return total\n",
        "\n",
        "\n",
        "\n",
        "# Esempio di uso\n",
        "p = 0.5  # Probabilità di vincere un punto\n",
        "gx = 5\n",
        "gy = 5\n",
        "x = 3   # Punti già vinti dal giocatore   1 = 15, 2 = 30, 3 = 40\n",
        "y = 2   # Punti già vinti dall'avversario\n",
        "'''\n",
        "result = prob(p, gx, gy, x, y)\n",
        "print(\"Probabilità:\", result)"
      ],
      "metadata": {
        "id": "q5OSuG7yGlab",
        "outputId": "1d90ca0a-4409-4e71-e268-eaa1f426bf78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after 'else' statement on line 16 (ipython-input-65-1168135103.py, line 17)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-65-1168135103.py\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    '''\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'else' statement on line 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model 2**"
      ],
      "metadata": {
        "id": "EnAZeV1LlUMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importiamo il dataset ottenuto tramite le operazioni di Data Preprocessing\n",
        "dataset = pd.read_csv(\"TT.csv\")\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "1Bnz1jzrkdIA",
        "outputId": "c247a745-1a2c-499c-c62b-d659c791fd4a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         player_id  opponent_id  Set1  Set2  points_progression  SetWinner  \\\n",
              "0           112463       117294     0     0                   2          2   \n",
              "1           112463       117294     0     0                   2          2   \n",
              "2           112463       117294     0     0                   1          2   \n",
              "3           112463       117294     0     0                   1          2   \n",
              "4           112463       117294     0     0                   2          2   \n",
              "...            ...          ...   ...   ...                 ...        ...   \n",
              "1222644     133652       133850     2     1                   1          1   \n",
              "1222645     133652       133850     2     1                   2          1   \n",
              "1222646     133652       133850     2     1                   2          1   \n",
              "1222647     133652       133850     2     1                   1          1   \n",
              "1222648     133652       133850     2     1                   1          1   \n",
              "\n",
              "         MatchWinner  event_match_id  PointWinner  PtSet  Pt   Pts  WonSetP1  \\\n",
              "0                  1               1            2      1   1   0-0         0   \n",
              "1                  1               1            2      2   2   0-1         0   \n",
              "2                  1               1            1      3   3   0-2         0   \n",
              "3                  1               1            1      4   4   1-2         0   \n",
              "4                  1               1            2      5   5   2-2         0   \n",
              "...              ...             ...          ...    ...  ..   ...       ...   \n",
              "1222644            1           19561            1     14  66   8-5         0   \n",
              "1222645            1           19561            2     15  67   9-5         0   \n",
              "1222646            1           19561            2     16  68   9-6         0   \n",
              "1222647            1           19561            1     17  69   9-7         0   \n",
              "1222648            1           19561            1     18  70  10-7         1   \n",
              "\n",
              "         WonSetP2  LostSetP1  LostSetP2  \n",
              "0               0          0          0  \n",
              "1               0          0          0  \n",
              "2               0          0          0  \n",
              "3               0          0          0  \n",
              "4               0          0          0  \n",
              "...           ...        ...        ...  \n",
              "1222644         0          0          0  \n",
              "1222645         0          0          0  \n",
              "1222646         0          0          0  \n",
              "1222647         0          0          0  \n",
              "1222648         0          0          0  \n",
              "\n",
              "[1222649 rows x 16 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a1c35153-8c71-4a4a-8c23-0e3523a321df\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>player_id</th>\n",
              "      <th>opponent_id</th>\n",
              "      <th>Set1</th>\n",
              "      <th>Set2</th>\n",
              "      <th>points_progression</th>\n",
              "      <th>SetWinner</th>\n",
              "      <th>MatchWinner</th>\n",
              "      <th>event_match_id</th>\n",
              "      <th>PointWinner</th>\n",
              "      <th>PtSet</th>\n",
              "      <th>Pt</th>\n",
              "      <th>Pts</th>\n",
              "      <th>WonSetP1</th>\n",
              "      <th>WonSetP2</th>\n",
              "      <th>LostSetP1</th>\n",
              "      <th>LostSetP2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>112463</td>\n",
              "      <td>117294</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0-0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>112463</td>\n",
              "      <td>117294</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>112463</td>\n",
              "      <td>117294</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0-2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>112463</td>\n",
              "      <td>117294</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1-2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>112463</td>\n",
              "      <td>117294</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2-2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1222644</th>\n",
              "      <td>133652</td>\n",
              "      <td>133850</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>19561</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>66</td>\n",
              "      <td>8-5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1222645</th>\n",
              "      <td>133652</td>\n",
              "      <td>133850</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>19561</td>\n",
              "      <td>2</td>\n",
              "      <td>15</td>\n",
              "      <td>67</td>\n",
              "      <td>9-5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1222646</th>\n",
              "      <td>133652</td>\n",
              "      <td>133850</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>19561</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>68</td>\n",
              "      <td>9-6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1222647</th>\n",
              "      <td>133652</td>\n",
              "      <td>133850</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>19561</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>69</td>\n",
              "      <td>9-7</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1222648</th>\n",
              "      <td>133652</td>\n",
              "      <td>133850</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>19561</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "      <td>70</td>\n",
              "      <td>10-7</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1222649 rows × 16 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a1c35153-8c71-4a4a-8c23-0e3523a321df')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a1c35153-8c71-4a4a-8c23-0e3523a321df button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a1c35153-8c71-4a4a-8c23-0e3523a321df');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-0f515267-80d2-43e7-bf4d-0d8e026164e9\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0f515267-80d2-43e7-bf4d-0d8e026164e9')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-0f515267-80d2-43e7-bf4d-0d8e026164e9 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_de147c3d-2b9f-4890-ab1b-763b2291bd33\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('dataset')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_de147c3d-2b9f-4890-ab1b-763b2291bd33 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('dataset');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dataset"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split 'Pts'\n",
        "dataset[['Pt1_raw', 'Pt2_raw']] = dataset['Pts'].astype(str).str.split('-', expand=True)\n",
        "\n",
        "score_map = {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, '10': 10, 'AD':11}\n",
        "\n",
        "dataset['Pt1'] = dataset['Pt1_raw'].map(score_map)\n",
        "dataset['Pt2'] = dataset['Pt2_raw'].map(score_map)\n",
        "dataset['SetID'] = dataset['event_match_id'].astype(str) + dataset['Set1'].astype(str) + dataset['Set2'].astype(str)\n",
        "\n",
        "\n",
        "# Definisci IsDeuce per game normale\n",
        "normal_deuce = (\n",
        "    (\n",
        "        ((dataset['Pt1'] == 10) & (dataset['Pt2'] == 10)) |   # 40-40\n",
        "        ((dataset['Pt1'] == 11) & (dataset['Pt2'] == 10)) |   # 40-ADV\n",
        "        ((dataset['Pt1'] == 10) & (dataset['Pt2'] == 11))     # ADV-40\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "# Combina i due casi\n",
        "dataset['IsDeuce'] = (normal_deuce).astype(int)\n",
        "\n",
        "dataset.drop(columns=['Pt1_raw', 'Pt2_raw','Pts','points_progression'], inplace=True)"
      ],
      "metadata": {
        "id": "WXL9pjgavJUZ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "jPQE1Hm9xgRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generazione delle partite inverse per aumentare la dimensione del dataset\n",
        "\n",
        "def augment_with_symmetric(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df_swapped = df.copy()\n",
        "\n",
        "    # Colonne da swappare a coppie\n",
        "    swap_pairs = [\n",
        "        ('Set1', 'Set2'),\n",
        "        ('Pt1', 'Pt2'),\n",
        "        ('player_id', 'opponent_id'),\n",
        "        ('WonSetP1', 'WonSetP2'),\n",
        "        ('LostSetP1', 'LostSetP2'),\n",
        "        ('player_id',\t'opponent_id')\n",
        "    ]\n",
        "\n",
        "    for col1, col2 in swap_pairs:\n",
        "        df_swapped[[col1, col2]] = df[[col2, col1]].values\n",
        "\n",
        "    # Colonne dove 1 <-> 2\n",
        "    invert_1_2_cols = ['PointWinner', 'SetWinner', 'MatchWinner']\n",
        "    for col in invert_1_2_cols:\n",
        "        df_swapped[col] = df[col].replace({1: 2, 2: 1})\n",
        "\n",
        "\n",
        "    # Aggiunta del suffisso 'simm' a match_id e set_id, se presenti\n",
        "    for col in ['event_match_id', 'SetID']:\n",
        "        if col in df_swapped.columns:\n",
        "            df_swapped[col] = df_swapped[col].astype(str) + '_simm'\n",
        "\n",
        "    return pd.concat([df, df_swapped], ignore_index=True)"
      ],
      "metadata": {
        "id": "KyQCME311ovY"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1️⃣ Scaling e Normalizzazione\n",
        "\n",
        "def preprocess_data(train_data, test_data):\n",
        "    # Copie di sicurezza\n",
        "    train_data = train_data.copy()\n",
        "    test_data = test_data.copy()\n",
        "\n",
        "    # === 1. Colonne da trasformare ===\n",
        "    minmax_cols = ['Pt1', 'Pt2']\n",
        "\n",
        "    others = ['PointWinner','WonSetP1',\t'WonSetP2'\t,'LostSetP1'\t,'LostSetP2', 'IsDeuce']\n",
        "\n",
        "    # === 2. Altre feature da conservare (numeriche/categoriche già pronte) ===\n",
        "    exclude_cols = minmax_cols +  others + \\\n",
        "                   ['match_id', 'SetID', 'Player1', 'Player2', 'PtSet', 'SetWinner', 'MatchWinner']\n",
        "\n",
        "    # === 3. Normalizzazione ===\n",
        "    minmax_scaler = MinMaxScaler()\n",
        "    train_data[minmax_cols] = minmax_scaler.fit_transform(train_data[minmax_cols])\n",
        "    test_data[minmax_cols] = minmax_scaler.transform(test_data[minmax_cols])\n",
        "\n",
        "\n",
        "    feature_cols = minmax_cols + others\n",
        "\n",
        "    # === 5. Target: conversione [1,2] → [0,1] ===\n",
        "    target_cols = ['PointWinner', 'SetWinner', 'MatchWinner']\n",
        "    for col in target_cols:\n",
        "        train_data = train_data[train_data[col].isin([1, 2])]\n",
        "        test_data = test_data[test_data[col].isin([1, 2])]\n",
        "        train_data[col] = train_data[col].astype(int) - 1\n",
        "        test_data[col] = test_data[col].astype(int) - 1\n",
        "\n",
        "    # === 6. Conversione finale a float ===\n",
        "    train_data[feature_cols] = train_data[feature_cols].astype(float)\n",
        "    test_data[feature_cols] = test_data[feature_cols].astype(float)\n",
        "\n",
        "    return train_data, test_data, feature_cols"
      ],
      "metadata": {
        "id": "hDOu9pIPzf9H"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2️⃣ Creazione sequenze\n",
        "\n",
        "def create_sequences(data, feature_cols, target_col='SetWinner'):\n",
        "    X_sequences = []\n",
        "    y_labels = []\n",
        "    lengths = []\n",
        "\n",
        "    # Ordina i punti all'interno di ciascun match\n",
        "    data = data.sort_values(['SetID', 'PtSet']).reset_index(drop=True)\n",
        "\n",
        "    for match_id, df_match in data.groupby('SetID'):\n",
        "        seq_data = df_match[feature_cols].to_numpy(dtype=np.float32)\n",
        "        X_seq = torch.tensor(seq_data, dtype=torch.float32)\n",
        "        X_sequences.append(X_seq)\n",
        "\n",
        "        # Prende il target del primo punto del match (assunto costante per tutti)\n",
        "        target_winner = int(df_match[target_col].iloc[0])\n",
        "        y_labels.append(target_winner)\n",
        "        lengths.append(len(df_match))\n",
        "\n",
        "    # Padding delle sequenze\n",
        "    X_padded = pad_sequence(X_sequences, batch_first=True, padding_value=0)\n",
        "    y_tensor = torch.tensor(y_labels, dtype=torch.float)\n",
        "    lengths = torch.tensor(lengths, dtype=torch.long)\n",
        "\n",
        "    return X_padded, y_tensor, lengths\n",
        "# 3️⃣ Pipeline completa + DataLoader\n",
        "\n",
        "def collate_fn(batch):\n",
        "    inputs, targets, lengths = zip(*batch)\n",
        "    inputs = torch.stack(inputs)\n",
        "    targets = torch.stack(targets)\n",
        "    lengths = torch.stack(lengths)\n",
        "    sorted_idx = torch.argsort(lengths, descending=True)\n",
        "    return inputs[sorted_idx], targets[sorted_idx], lengths[sorted_idx]\n",
        "\n",
        "def prepare_datasets(train_data, test_data, target_col='SetWinner', batch_size=32, val_split=0.1, random_state=42):\n",
        "    # Preprocessing dati\n",
        "    train_proc, test_proc, feature_cols = preprocess_data(train_data, test_data)\n",
        "\n",
        "    # Split ulteriore del training set in train/validation\n",
        "    match_ids = train_proc['SetID'].unique()\n",
        "    train_ids, val_ids = train_test_split(match_ids, test_size=val_split, random_state=random_state)\n",
        "\n",
        "    train_split = train_proc[train_proc['SetID'].isin(train_ids)]\n",
        "    val_split = train_proc[train_proc['SetID'].isin(val_ids)]\n",
        "\n",
        "    # Creazione sequenze per train/val/test\n",
        "    X_train, y_train, train_lengths = create_sequences(train_split, feature_cols, target_col)\n",
        "    X_val, y_val, val_lengths = create_sequences(val_split, feature_cols, target_col)\n",
        "    X_test, y_test, test_lengths = create_sequences(test_proc, feature_cols, target_col)\n",
        "\n",
        "    # Costruzione dei TensorDataset\n",
        "    train_dataset = TensorDataset(X_train, y_train, train_lengths)\n",
        "    val_dataset = TensorDataset(X_val, y_val, val_lengths)\n",
        "    test_dataset = TensorDataset(X_test, y_test, test_lengths)\n",
        "\n",
        "    # DataLoader\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                             shuffle=True, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size,\n",
        "                           shuffle=False, collate_fn=collate_fn)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
        "                            shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    return train_loader, val_loader, test_loader, feature_cols"
      ],
      "metadata": {
        "id": "te7CJaxi0aCU"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------- 1. MODELLO AGGIORNATO ----------\n",
        "class LSTMPerStep(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers,\n",
        "                            batch_first=True, bidirectional=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.output_layer = nn.Linear(hidden_size, 1)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def forward(self, x, lengths, hidden=None):\n",
        "        lengths = lengths.cpu()\n",
        "        packed = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
        "        packed_output, hidden = self.lstm(packed, hidden)\n",
        "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
        "        output = self.dropout(output)\n",
        "        logits = self.output_layer(output)  # (batch, seq_len, 1)\n",
        "        probs = torch.sigmoid(logits).squeeze(-1)  # (batch, seq_len)\n",
        "        return probs, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size, device):\n",
        "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device),\n",
        "                torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device))\n",
        "\n",
        "# ---------- 2. NUOVA LOSS PESATA ----------\n",
        "def exponential_weighted_loss(y_pred, y_true, lengths, device, final_weight=10.0):\n",
        "    batch_size, max_len = y_pred.shape\n",
        "    loss = F.binary_cross_entropy(y_pred, y_true, reduction='none')\n",
        "\n",
        "    weights = torch.zeros_like(y_pred, device=device)\n",
        "    mask = torch.arange(max_len, device=device)[None, :] < lengths[:, None]\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        L = lengths[i].item()\n",
        "        t = torch.arange(0, L, device=device).float()\n",
        "        sequence_weights = torch.exp(final_weight * (t / L - 1))\n",
        "        sequence_weights = sequence_weights / sequence_weights.mean()\n",
        "        weights[i, :L] = sequence_weights\n",
        "\n",
        "    weighted_loss = loss * weights\n",
        "\n",
        "    # Correzione definitiva: seleziona SOLO i valori validi usando la maschera\n",
        "    valid_loss = weighted_loss[mask].mean()\n",
        "\n",
        "    return valid_loss\n",
        "\n",
        "# ---------- DATA AUGMENTATION ----------\n",
        "def partial_sequence_augmentation(batch_X, batch_y, lengths, prob=0.3):\n",
        "    batch_size, max_len, feat_dim = batch_X.size()\n",
        "    for i in range(batch_size):\n",
        "        if torch.rand(1).item() < prob and lengths[i] > 5:\n",
        "            new_len = int(lengths[i].item() * torch.rand(1).item() * 0.4 + lengths[i].item() * 0.5)\n",
        "            lengths[i] = new_len\n",
        "            batch_X[i, new_len:] = 0\n",
        "    return batch_X, batch_y, lengths\n",
        "\n",
        "# ---------- AGGIORNAMENTO TRAINING LOOP ----------\n",
        "def train(model, train_loader, val_loader, optimizer, device, epochs=50, patience=5):\n",
        "    best_val_loss = float('inf')\n",
        "    counter = 0\n",
        "    history = {'train_loss': [], 'val_loss': []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for X_batch, y_batch, lengths in train_loader:\n",
        "            X_batch, y_batch, lengths = partial_sequence_augmentation(X_batch, y_batch, lengths.clone())\n",
        "            actual_batch_size = X_batch.size(0)\n",
        "            hidden = model.init_hidden(actual_batch_size, device)\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            lengths = lengths.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            preds, _ = model(X_batch, lengths)\n",
        "            y_expanded = y_batch.unsqueeze(1).expand(-1, preds.size(1))\n",
        "\n",
        "            loss = exponential_weighted_loss(preds, y_expanded, lengths, device)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        val_loss = 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch, lengths in val_loader:\n",
        "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "                lengths = lengths.to(device)\n",
        "\n",
        "                preds, _ = model(X_batch, lengths)\n",
        "                y_expanded = y_batch.unsqueeze(1).expand(-1, preds.size(1))\n",
        "                loss = exponential_weighted_loss(preds, y_expanded, lengths, device)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        val_loss /= len(val_loader)\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            counter = 0\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "        else:\n",
        "            counter += 1\n",
        "            if counter >= patience:\n",
        "                print(f'Early stopping at epoch {epoch+1}')\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "    return history\n",
        "\n",
        "\n",
        "# ---------- 5. ESEMPIO DI UTILIZZO AGGIORNATO ----------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_size = 8\n",
        "hidden_size = 64\n",
        "\n",
        "model = LSTMPerStep(input_size=input_size, hidden_size=hidden_size).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "# Preparazione dati (assumendo che 'dataset' sia già definito)\n",
        "match_ids = dataset['event_match_id'].unique()\n",
        "train_ids, test_ids = train_test_split(match_ids, test_size=0.2, random_state=42)\n",
        "train_data = dataset[dataset['event_match_id'].isin(train_ids)]\n",
        "test_data = dataset[dataset['event_match_id'].isin(test_ids)]\n",
        "\n",
        "train_data = augment_with_symmetric(train_data)\n",
        "test_data = augment_with_symmetric(test_data)\n",
        "\n",
        "# Creazione DataLoader (implementa la tua funzione prepare_datasets)\n",
        "train_loader, val_loader, test_loader, feature_cols = prepare_datasets(\n",
        "    train_data, test_data, target_col='SetWinner', batch_size=32, val_split=0.1\n",
        ")\n",
        "\n",
        "# Allenamento\n",
        "history = train(model, train_loader, val_loader, optimizer, device, epochs=100)\n",
        "\n",
        "# Plot della loss\n",
        "plt.plot(history['train_loss'], label='Train Loss')\n",
        "plt.plot(history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training Progress')\n",
        "plt.show()\n",
        "\n",
        "# Predizione su una sequenza di test\n",
        "X_test, y_test, lengths = next(iter(test_loader))\n",
        "X_seq = X_test[0][:lengths[0]]  # Sequenza completa\n",
        "\n",
        "print(f\"Etichetta vera: {int(y_test[0].item())}\")\n",
        "full_probs = predict_sequence_steps(model, X_seq, device)\n",
        "\n",
        "# Plot evoluzione probabilità\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, len(full_probs) + 1), full_probs, marker='o')\n",
        "plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5)\n",
        "plt.xlabel(\"Numero di punti osservati\")\n",
        "plt.ylabel(\"Probabilità predetta\")\n",
        "plt.title(\"Evoluzione della predizione\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SGFV0gDC0s73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prob(p, x, y):\n",
        "\n",
        "    # Caso base: il giocatore ha già vinto 11 e con almeno 2 di vantaggio\n",
        "    if x >= 11 and x - y >= 2:\n",
        "        return 1.0\n",
        "    elif y >= 11 and y - x >= 2:\n",
        "        return 0.0\n",
        "\n",
        "    '''\n",
        "    # Calcola probabilità da situazione di Deuce\n",
        "    d = (p ** 2) / (1 - 2 * p * (1 - p))\n",
        "\n",
        "    # Caso Advantage giocatore A (avanti di 1 punto)\n",
        "    if x >= 10 and y >= 10 and x - y == 1:\n",
        "        return p + (1 - p) * d\n",
        "\n",
        "    # Caso Advantage giocatore B (A è sotto di 1 punto)\n",
        "    if x >= 10 and y >= 10 and x - y == -1:\n",
        "        return p * d\n",
        "\n",
        "    # Caso Deuce (pareggio)\n",
        "    if x >= 10 and y >= 10 and x - y == 0:\n",
        "        return d\n",
        "    '''\n",
        "\n",
        "    sum_part1 = 0\n",
        "    for i in range(0, 10 - y):\n",
        "        sum_part1 += p**(11 - x) * math.comb(10 + i - x, i) * (1 - p)**i\n",
        "\n",
        "    # Parte via deuce da 10-10\n",
        "    part2 = p**(10 - x) * math.comb(20 - x - y, 10 - x) * (1 - p)**(10 - y)\n",
        "    d = (p**2) / (1 - 2 * p * (1 - p))  # Probabilità di vincere da deuce\n",
        "    part2 *= d\n",
        "\n",
        "    total = sum_part1 + part2\n",
        "    return total\n",
        "\n",
        "# Esempio di uso\n",
        "p = 0.5  # Probabilità di vincere un punto\n",
        "x = 1   # Punti già vinti dal giocatore\n",
        "y = 10   # Punti già vinti dall'avversario\n",
        "\n",
        "result = prob(p, x, y)\n",
        "print(\"Probabilità:\", result)"
      ],
      "metadata": {
        "id": "xjKONqDA4NO5",
        "outputId": "7300c001-a6ea-4b2d-ec37-17d7480ad04b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilità: 0.0009765625\n"
          ]
        }
      ]
    }
  ]
}